{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cortex Stage A1 Training - Google Colab\n",
        "\n",
        "This notebook trains Cortex fast-weight sidecars on synthetic long-context tasks.\n",
        "\n",
        "**Requirements:** T4 GPU (free tier) or better\n",
        "\n",
        "**Runtime:** ~1-2 hours for full training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q torch>=2.0.0 transformers>=4.30.0 accelerate sentencepiece pyyaml matplotlib numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-detect environment and setup paths\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Smart detection: check which cortex-4 path actually exists\n",
        "if os.path.exists('/Users/mazalcohen/cortex-4'):\n",
        "    # Running locally\n",
        "    IN_COLAB = False\n",
        "    CORTEX_ROOT = '/Users/mazalcohen/cortex-4'\n",
        "    CHECKPOINT_DIR = '/Users/mazalcohen/cortex-4/checkpoints'\n",
        "    LOG_DIR = '/Users/mazalcohen/cortex-4/logs/a1'\n",
        "elif os.path.exists('/content/cortex-4'):\n",
        "    # Running in Google Colab cloud\n",
        "    IN_COLAB = True\n",
        "    CORTEX_ROOT = '/content/cortex-4'\n",
        "    CHECKPOINT_DIR = '/content/checkpoints'\n",
        "    LOG_DIR = '/content/logs/a1'\n",
        "else:\n",
        "    raise FileNotFoundError(\"cortex-4 folder not found at /Users/mazalcohen/cortex-4 or /content/cortex-4\")\n",
        "\n",
        "# Add cortex-4 to path\n",
        "sys.path.insert(0, CORTEX_ROOT)\n",
        "\n",
        "# Display config\n",
        "print(f\"Environment: {'Google Colab (cloud)' if IN_COLAB else 'Local Colab'}\")\n",
        "print(f\"Cortex root: {CORTEX_ROOT}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"Logs: {LOG_DIR}\")\n",
        "print(\"âœ“ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-Configure Based on GPU Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {gpu_mem_gb:.1f} GB\\n\")\n",
        "    \n",
        "    # Auto-config based on available memory\n",
        "    if gpu_mem_gb > 14:\n",
        "        GAPS = [512, 1024, 2048]\n",
        "        BATCH_SIZE = 2\n",
        "        SAMPLES = 256\n",
        "        print(\"Using MEDIUM config (T4/16GB)\")\n",
        "    elif gpu_mem_gb > 10:\n",
        "        GAPS = [256, 512, 1024]\n",
        "        BATCH_SIZE = 1\n",
        "        SAMPLES = 128\n",
        "        print(\"Using SMALL config (12GB)\")\n",
        "    else:\n",
        "        GAPS = [256, 512]\n",
        "        BATCH_SIZE = 1\n",
        "        SAMPLES = 64\n",
        "        print(\"Using MINIMAL config (<10GB)\")\n",
        "    \n",
        "    print(f\"  Gaps: {GAPS}\")\n",
        "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"  Samples per gap: {SAMPLES}\")\n",
        "else:\n",
        "    print(\"ERROR: No GPU detected! Enable GPU in Runtime > Change runtime type\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Stage A1 Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(CORTEX_ROOT)\n",
        "print(f\"Working directory: {os.getcwd()}\\n\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "gaps_str = ' '.join(map(str, GAPS))\n",
        "\n",
        "print(f\"Starting Stage A1 Training:\")\n",
        "print(f\"  Task: Key-Value Binding\")\n",
        "print(f\"  Gaps: {GAPS}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Samples per gap: {SAMPLES}\")\n",
        "print(f\"  Model: Qwen/Qwen1.5-1.8B-Chat\")\n",
        "print(f\"  Output: {CHECKPOINT_DIR}\")\n",
        "print(f\"  Logs: {LOG_DIR}\\n\")\n",
        "\n",
        "!python scripts/stage_a1_enable_fast.py \\\n",
        "    --model Qwen/Qwen1.5-1.8B-Chat \\\n",
        "    --task kv \\\n",
        "    --gaps {gaps_str} \\\n",
        "    --batch_size {BATCH_SIZE} \\\n",
        "    --epochs 2 \\\n",
        "    --save_dir {CHECKPOINT_DIR} \\\n",
        "    --log_dir {LOG_DIR} \\\n",
        "    --amp true \\\n",
        "    --samples_per_gap {SAMPLES} \\\n",
        "    --seed 42 \\\n",
        "    --fast_rank 16 \\\n",
        "    --lr_sidecar 2e-4 \\\n",
        "    --grad_clip 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "probe_files = list(Path(LOG_DIR).rglob('probes.jsonl'))\n",
        "if probe_files:\n",
        "    probe_file = probe_files[0]\n",
        "    print(f\"Analyzing: {probe_file}\\n\")\n",
        "    \n",
        "    gap_accuracy = defaultdict(list)\n",
        "    gap_fast_share = defaultdict(list)\n",
        "    \n",
        "    with open(probe_file) as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            gap = record['gap']\n",
        "            gap_accuracy[gap].append(record['correct'])\n",
        "            gap_fast_share[gap].append(record.get('fast_share_mean', 0))\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"ACCURACY BY GAP LENGTH:\")\n",
        "    print(\"=\" * 60)\n",
        "    for gap in sorted(gap_accuracy.keys()):\n",
        "        acc_list = gap_accuracy[gap]\n",
        "        acc = sum(acc_list) / len(acc_list)\n",
        "        fast_share = sum(gap_fast_share[gap]) / len(gap_fast_share[gap])\n",
        "        print(f\"Gap {gap:4d}: {acc*100:5.1f}% ({sum(acc_list):3d}/{len(acc_list):3d}) | Fast Share: {fast_share:.3f}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(f\"No probe logs found in {LOG_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Archive results\n",
        "import os\n",
        "result_zip = os.path.join(os.path.dirname(CORTEX_ROOT), 'cortex_a1_results.zip')\n",
        "\n",
        "print(f\"Archiving results to: {result_zip}\")\n",
        "!zip -r {result_zip} {LOG_DIR} {CHECKPOINT_DIR}\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download(result_zip)\n",
        "    print(\"\\nDownload started! Check your browser's download folder.\")\n",
        "else:\n",
        "    print(f\"\\nResults saved to: {result_zip}\")\n",
        "    print(\"You can find your logs and checkpoints in:\")\n",
        "    print(f\"  - Logs: {LOG_DIR}\")\n",
        "    print(f\"  - Checkpoints: {CHECKPOINT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
